{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the supplementary material of Ba, Kiros, Hinton, 2016. These are their equations for LSTM and layer normalized LSTM:\n",
    "(edited a little so they would display in the notebook environment).\n",
    "\n",
    "This section describes how layer normalization is applied to each of the papers'\n",
    " experiments. For notation convenience, we define layer normalization as a funct\n",
    "ion mapping $LN: R^D \\to R^D$ with two set of adaptive parameters, gains\n",
    " ${\\bf \\alpha }$ and biases ${\\bf \\beta }$ :\n",
    " \n",
    "\\begin{eqnarray}\n",
    "LN({\\bf z} ; \\bf{\\alpha}, \\bf{\\beta}) = \\frac{( z - \\mu)}{\\sigma} \\odot \\bf\n",
    "{\\alpha} + \\bf{\\beta}, \\\\\n",
    "\\mu = \\frac{1}{D}\\sum_{i=1}^D z_i, \\quad \\sigma = \\sqrt{\\frac{1}{D}\\sum_{i=1}^D\n",
    "(z_i-\\mu)^2},\n",
    "\\end{eqnarray}\n",
    "where, $z_i$ is the $i^{th}$ element of the vector ${\\bf z}$.\n",
    "\n",
    "\n",
    "\n",
    "The basic LSTM equations used for these experiment are given by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}{\\bf f}_t\\\\{\\bf i}_t\\\\{\\bf o}_t\\\\{\\bf g}_t\\end{pmatrix} &=& {\\bf\n",
    "W}_h {\\bf h}_{t-1} + {\\bf W}_x {\\bf x}_t + b \\\\\n",
    "{\\bf c}_t &=& \\sigma({\\bf f}_t) \\odot {\\bf c}_{t-1} + \\sigma({\\bf i}_t) \\odot \\tanh({\\bf g}_t) \\\\\n",
    "{\\bf h}_t &=& \\sigma({\\bf o}_t) \\odot \\text{tanh}({\\bf c}_t)\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version that incorporates layer normalization is modified as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}{\\bf f}_t\\\\{\\bf i}_t\\\\{\\bf o}_t\\\\{\\bf g}_t\\end{pmatrix} &=& LN({\\bf W}_h {\\bf h}_{t-1}; \\bf{\\alpha}_1, \\bf{\\beta}_1) + LN({\\bf W}_x {\\bf x}_t; \\bf{\\alpha}_2, \\bf{\\beta}_2) + b \\\\\n",
    "{\\bf c}_t &=& \\sigma({\\bf f}_t) \\odot {\\bf c}_{t-1} + \\sigma({\\bf i}_t) \\odot \\text{tanh}({\\bf g}_t) \\\\\n",
    "{\\bf h}_t &=& \\sigma({\\bf o}_t) \\odot \\text{tanh}(LN({\\bf c}_t; \\bf{\\alpha}_3, \\bf{\\beta}_3))\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\alpha_i, \\beta_i$ are the additive and multiplicative parameters, respectively. Each $\\bf{\\alpha}_i$ is initialized to a vector of zeros and each $\\bf{\\beta}_i$ is initialized to a vector of ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeled after ryankiros and gruln code\n",
    "# a simple sample normalization code, which I've tested with keras\n",
    "# this does not also apply the parameters like ryankiros's code does\n",
    "\n",
    "def sample_normalize(x, _eps=1e-5):\n",
    "    \"\"\"centers a set of samples x to have zero mean and unit standard deviation\"\"\"\n",
    "    # keepdims=True the axes which are reduced are left in the result as dimensions with size one\n",
    "    # axis=-1 means do things across the last axis\n",
    "    m = K.mean(x, axis=-1, keepdims=True) # could subtract this off earlier\n",
    "    # std = K.std(x)\n",
    "    std = K.sqrt(K.var(x, axis=-1, keepdims=True) + _eps) # not using K.std for _eps stability\n",
    "    return (x-m)/ (std+_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the code for a single step of LSTM looks like in Keras \n",
    "(recurrent.py, the LSTM class function)\n",
    "It looks like the keras implementation divides the weigths W into feedforward connections $W$ and recurrent connections $U$. That looks like it stored in a large concatenated matrices self.W, self.U.\n",
    "It uses z's as the symbols for the linear (pre-activation function) values instead of the Ba paper formalization while the letters $i, f, o$ are post-activation function gate values. And, instead of using $g_t$ as the total for the linear pre-activation to $c_t$, it uses $z2$\n",
    "\n",
    "```python \n",
    "    def step(self, x, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        B_U = states[2]\n",
    "        B_W = states[3]\n",
    "\n",
    "        if self.consume_less == 'gpu':\n",
    "            z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n",
    "\n",
    "            z0 = z[:, :self.output_dim]\n",
    "            z1 = z[:, self.output_dim: 2 * self.output_dim]\n",
    "            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "            z3 = z[:, 3 * self.output_dim:]\n",
    "\n",
    "            i = self.inner_activation(z0)\n",
    "            f = self.inner_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.inner_activation(z3)\n",
    "        else:\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_i = x[:, :self.output_dim]\n",
    "                x_f = x[:, self.output_dim: 2 * self.output_dim]\n",
    "                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "                x_o = x[:, 3 * self.output_dim:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n",
    "                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n",
    "                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n",
    "                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n",
    "            else:\n",
    "                raise Exception('Unknown `consume_less` mode.')\n",
    "\n",
    "            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n",
    "            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n",
    "            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "```\n",
    "\n",
    "It is not totally clear to me why this particular degree of normalization was chosen for LSTM. I could imagine for instance just normalizing the feedforward inputs ininitially $x_t -> LN(x_t)$, and I think it would be a good idea to test out this with a comparison. Instead there are multiple recenterings going: before the output is generated, the input from the memory cells is normalized again, essentially treating it as a separate layer.\n",
    "\n",
    "[An open question for myself is how such normalization affect network learning when changes in the overall level of inputs is important (such as in physiological signals).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the ln() normalization function from ryankiros (written in theano). It both normalizes the tensor x but also \n",
    "multiplies in the s Tensor and adds the bias term b\n",
    "\n",
    "```python\n",
    "def ln(x, b, s):\n",
    "    _eps = 1e-5\n",
    "    output = (x - x.mean(1)[:,None]) / tensor.sqrt((x.var(1)[:,None] + _eps))\n",
    "    output = s[None, :] * output + b[None,:]\n",
    "    return output\n",
    "```\n",
    "\n",
    "And here is the _step function:\n",
    "- he separates inputs into sbelow and sbefore (sbefore starts at initial state)\n",
    "- each sbelow and sbefore get normalized\n",
    "- the memory cell actictivity c also is normalized as well\n",
    "\n",
    "```python\n",
    "# class function for lstm layser normalization\n",
    "\n",
    "\n",
    "\n",
    "    def _step(mask, sbelow, sbefore, cell_before, *args):\n",
    "        sbelow_ = ln(sbelow, param('b1'), param('s1'))\n",
    "        sbefore_ = ln(dot(sbefore, param('U')), param('b2'), param('s2'))\n",
    "\n",
    "        preact = sbefore_ + sbelow_ + param('b')\n",
    "\n",
    "        i = Sigmoid(_slice(preact, 0, dim))\n",
    "        f = Sigmoid(_slice(preact, 1, dim))\n",
    "        o = Sigmoid(_slice(preact, 2, dim))\n",
    "        c = Tanh(_slice(preact, 3, dim))\n",
    "\n",
    "        c = f * cell_before + i * c\n",
    "        c = mask * c + (1. - mask) * cell_before\n",
    "\n",
    "        c_ = ln(c, param('b3'), param('s3'))\n",
    "        h = o * tensor.tanh(c_)\n",
    "        h = mask * h + (1. - mask) * sbefore\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_LN(LSTM):\n",
    "    \"\"\"\n",
    "    LSTM with layer normalization\n",
    "    adds parameters alpha_i (multiplicative), beta_i (additive)\n",
    "    which are initialized to ones and zeros vectors of same length as the inputs\n",
    "    see above function \n",
    "    \"\"\"\n",
    "    \n",
    "    def laynorm(self, x):  # ignore alpha and beta for starters, fix alpha = 1.0, beta = 0.0\n",
    "        \"\"\"centers a set of samples x to have zero mean and unit standard deviation\"\"\"\n",
    "        # keepdims=True the axes which are reduced are left in the result as dimensions with size one\n",
    "        # axis=-1 means do things across the last axis\n",
    "        m = K.mean(x, axis=-1, keepdims=True) # could subtract this off earlier\n",
    "        # std = K.std(x)\n",
    "        std = K.sqrt(K.var(x, axis=-1, keepdims=True) + _eps) # not using K.std for _eps stability\n",
    "        output = (x-m)/ (std+_eps)\n",
    "        # output = alpha * output + beta\n",
    "        # output = alpha * output\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def step(self, x, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        B_U = states[2]\n",
    "        B_W = states[3]\n",
    "\n",
    "        if self.consume_less == 'gpu':\n",
    "            # original linear activity\n",
    "            # z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n",
    "            # linear activity without bias term self.b # will need to add this back !!! (see what ryankiros does in ln())\n",
    "\n",
    "            ## question: what is B_W[0] for ??? note that it is is may be used in dropout in get_constants() function\n",
    "            \n",
    "            # todo: double check that adding self.b here is same as doing LN on each as in ryankiros implementation\n",
    "            # this may not actually work\n",
    "            z = self.laynorm(K.dot(x * B_W[0], self.W)) + self.laynorm(K.dot(h_tm1 * B_U[0], self.U)) + self.b\n",
    "            # seems that ryankiros divides things into inputs from below and recurrent input from before (t-1)\n",
    "            # and normalizes them \n",
    "            \n",
    "            z0 = z[:, :self.output_dim]                         # i_t in Ba2016       \n",
    "            z1 = z[:, self.output_dim: 2 * self.output_dim]     # f_t in Ba2016\n",
    "            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim] # g_t in Ba2016\n",
    "            z3 = z[:, 3 * self.output_dim:]                     # o_t in Ba2016\n",
    "            # normalization\n",
    "            \n",
    "            i = self.inner_activation(z0)    # \\sigma(i_t)\n",
    "            f = self.inner_activation(z1)    # \\sigma(f_t)\n",
    "            c = f * c_tm1 + i * self.activation(z2)  # c_t = sigma(f_t) .* c_{t-1} + \\sigma(i_t) .* tanh(g_t)\n",
    "            o = self.inner_activation(z3) # \\sigma(o_t) in Ba2016\n",
    "            \n",
    "        ## have not applied layer normalization to these code paths yet:: -clm    \n",
    "        else: # CLM: now this does not make sense to me for example x_i is something different depending on 'cpu' vs 'mem'\n",
    "            if self.consume_less == 'cpu':  # bug? I do not see W ever being applied to the X along the 'cpu' path. \n",
    "                # maybe the W is applied in preprocess_input? that must be it\n",
    "                x_i = x[:, :self.output_dim]\n",
    "                x_f = x[:, self.output_dim: 2 * self.output_dim]\n",
    "                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "                x_o = x[:, 3 * self.output_dim:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n",
    "                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n",
    "                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n",
    "                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n",
    "            else:\n",
    "                raise Exception('Unknown `consume_less` mode.')\n",
    "\n",
    "            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n",
    "            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n",
    "            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n",
    "\n",
    "        h = o * self.activation(self.laynorm(c))\n",
    "        return h, [h, c]\n",
    "    \n",
    "    def preprocess_input(self, x):\n",
    "        if self.consume_less == 'cpu':\n",
    "            if 0 < self.dropout_W < 1:\n",
    "                dropout = self.dropout_W\n",
    "            else:\n",
    "                dropout = 0\n",
    "            input_shape = self.input_spec[0].shape\n",
    "            input_dim = input_shape[2]\n",
    "            timesteps = input_shape[1]\n",
    "            # clm:: Apply y.w + b for every temporal slice y of x.\n",
    "            x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n",
    "                                         input_dim, self.output_dim, timesteps)\n",
    "            x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n",
    "                                         input_dim, self.output_dim, timesteps)\n",
    "            x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n",
    "                                         input_dim, self.output_dim, timesteps)\n",
    "            x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n",
    "                                         input_dim, self.output_dim, timesteps)\n",
    "            return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
