{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.layers.LSTM as LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the supplementary material of Ba, Kiros, Hinton, 2016. These are their equations for LSTM and layer normalized LSTM:\n",
    "(edited a little so they would display in the notebook environment).\n",
    "\n",
    "This section describes how layer normalization is applied to each of the papers'\n",
    " experiments. For notation convenience, we define layer normalization as a funct\n",
    "ion mapping $LN: R^D \\to R^D$ with two set of adaptive parameters, gains\n",
    " ${\\bf \\alpha }$ and biases ${\\bf \\beta }$ :\n",
    "\\begin{eqnarray}\n",
    "LN({\\bf z} ; \\bf{\\alpha}, \\bf{\\beta}) = \\frac{({\\bf z} - \\mu)}{\\sigma} \\odot \\bf\n",
    "{\\alpha} + \\bf{\\beta}, \\\\\n",
    "\\mu = \\frac{1}{D}\\sum_{i=1}^D z_i, \\quad \\sigma = \\sqrt{\\frac{1}{D}\\sum_{i=1}^D\n",
    "(z_i-\\mu)^2},\n",
    "\\end{eqnarray}\n",
    "where, $z_i$ is the $i^{th}$ element of the vector ${\\bf z}$.\n",
    "\n",
    "\n",
    "\n",
    "The basic LSTM equations used for these experiment are given by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}{\\bf f}_t\\\\{\\bf i}_t\\\\{\\bf o}_t\\\\{\\bf g}_t\\end{pmatrix} &=& {\\bf\n",
    "W}_h {\\bf h}_{t-1} + {\\bf W}_x {\\bf x}_t + b \\\\\n",
    "{\\bf c}_t &=& \\sigma({\\bf f}_t) \\odot {\\bf c}_{t-1} + \\sigma({\\bf i}_t) \\odot \\tanh({\\bf g}_t) \\\\\n",
    "{\\bf h}_t &=& \\sigma({\\bf o}_t) \\odot \\text{tanh}({\\bf c}_t)\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version that incorporates layer normalization is modified as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{pmatrix}{\\bf f}_t\\\\{\\bf i}_t\\\\{\\bf o}_t\\\\{\\bf g}_t\\end{pmatrix} &=& LN({\\bf W}_h {\\bf h}_{t-1}; \\bf{\\alpha}_1, \\bf{\\beta}_1) + LN({\\bf W}_x {\\bf x}_t; \\bf{\\alpha}_2, \\bf{\\beta}_2) + b \\\\\n",
    "{\\bf c}_t &=& \\sigma({\\bf f}_t) \\odot {\\bf c}_{t-1} + \\sigma({\\bf i}_t) \\odot \\text{tanh}({\\bf g}_t) \\\\\n",
    "{\\bf h}_t &=& \\sigma({\\bf o}_t) \\odot \\text{tanh}(LN({\\bf c}_t; \\bf{\\alpha}_3, \\bf{\\beta}_3))\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\bf{\\alpha}_i, \\bf{\\beta}_i$ are the additive and multiplicative parameters, respectively. Each $\\bf{\\alpha}_i$ is initialized to a vector of zeros and each $\\bf{\\beta}_i$ is initialized to a vector of ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeled after ryankiros and gruln code\n",
    "# a simple sample normalization code, which I've tested with keras\n",
    "# this does not also apply the parameters like ryankiros's code does\n",
    "\n",
    "def sample_normalize(x, _eps=1e-5):\n",
    "    \"\"\"centers a set of samples x to have zero mean and unit standard deviation\"\"\"\n",
    "    # keepdims=True the axes which are reduced are left in the result as dimensions with size one\n",
    "    # axis=-1 means do things across the last axis\n",
    "    m = K.mean(x, axis=-1, keepdims=True) # could subtract this off earlier\n",
    "    # std = K.std(x)\n",
    "    std = K.sqrt(K.var(x, axis=-1, keepdims=True) + _eps) # not using K.std for _eps stability\n",
    "    return (x-m)/ (std+_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the code for a single step of LSTM looks like in Keras \n",
    "(recurrent.py, the LSTM class function)\n",
    "\n",
    "```python \n",
    "    def step(self, x, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        B_U = states[2]\n",
    "        B_W = states[3]\n",
    "\n",
    "        if self.consume_less == 'gpu':\n",
    "            z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n",
    "\n",
    "            z0 = z[:, :self.output_dim]\n",
    "            z1 = z[:, self.output_dim: 2 * self.output_dim]\n",
    "            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "            z3 = z[:, 3 * self.output_dim:]\n",
    "\n",
    "            i = self.inner_activation(z0)\n",
    "            f = self.inner_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.inner_activation(z3)\n",
    "        else:\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_i = x[:, :self.output_dim]\n",
    "                x_f = x[:, self.output_dim: 2 * self.output_dim]\n",
    "                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "                x_o = x[:, 3 * self.output_dim:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n",
    "                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n",
    "                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n",
    "                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n",
    "            else:\n",
    "                raise Exception('Unknown `consume_less` mode.')\n",
    "\n",
    "            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n",
    "            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n",
    "            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the ln() normalization function from ryankiros (written in theano). It both normalizes the tensor x but also \n",
    "multiplies in the s Tensor and adds the bias term b\n",
    "\n",
    "```python\n",
    "def ln(x, b, s):\n",
    "    _eps = 1e-5\n",
    "    output = (x - x.mean(1)[:,None]) / tensor.sqrt((x.var(1)[:,None] + _eps))\n",
    "    output = s[None, :] * output + b[None,:]\n",
    "    return output\n",
    "```\n",
    "\n",
    "And here is the _step function:\n",
    "- he separates inputs into sbelow and sbefore (sbefore starts at initial state)\n",
    "- each sbelow and sbefore get normalized\n",
    "- the memory cell actictivity c also is normalized as well\n",
    "\n",
    "```python\n",
    "# class function for lstm layser normalization\n",
    "\n",
    "\n",
    "\n",
    "    def _step(mask, sbelow, sbefore, cell_before, *args):\n",
    "        sbelow_ = ln(sbelow, param('b1'), param('s1'))\n",
    "        sbefore_ = ln(dot(sbefore, param('U')), param('b2'), param('s2'))\n",
    "\n",
    "        preact = sbefore_ + sbelow_ + param('b')\n",
    "\n",
    "        i = Sigmoid(_slice(preact, 0, dim))\n",
    "        f = Sigmoid(_slice(preact, 1, dim))\n",
    "        o = Sigmoid(_slice(preact, 2, dim))\n",
    "        c = Tanh(_slice(preact, 3, dim))\n",
    "\n",
    "        c = f * cell_before + i * c\n",
    "        c = mask * c + (1. - mask) * cell_before\n",
    "\n",
    "        c_ = ln(c, param('b3'), param('s3'))\n",
    "        h = o * tensor.tanh(c_)\n",
    "        h = mask * h + (1. - mask) * sbefore\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM-LN(LSTM):\n",
    "    def step(self, x, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        B_U = states[2]\n",
    "        B_W = states[3]\n",
    "\n",
    "        if self.consume_less == 'gpu':\n",
    "            # original linear activity\n",
    "            # z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n",
    "            # linear activity without bias term self.b # will need to add this back !!! (see what ryankiros does in ln())\n",
    "\n",
    "            z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U)\n",
    "            # seems that ryankiros divides things into inputs from below and recurrent input from before (t-1)\n",
    "            # and normalizes them \n",
    "            \n",
    "            z0 = z[:, :self.output_dim]                         # z0(x_i)         \n",
    "            z1 = z[:, self.output_dim: 2 * self.output_dim]     # z1(x_f)\n",
    "            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim] # z2(x_c)\n",
    "            z3 = z[:, 3 * self.output_dim:]                     # z3(x_o)\n",
    "            # normalization\n",
    "            \n",
    "            i = self.inner_activation(z0)\n",
    "            f = self.inner_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.inner_activation(z3)\n",
    "        else:\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_i = x[:, :self.output_dim]\n",
    "                x_f = x[:, self.output_dim: 2 * self.output_dim]\n",
    "                x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n",
    "                x_o = x[:, 3 * self.output_dim:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n",
    "                x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n",
    "                x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n",
    "                x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n",
    "            else:\n",
    "                raise Exception('Unknown `consume_less` mode.')\n",
    "\n",
    "            i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n",
    "            f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n",
    "            o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
